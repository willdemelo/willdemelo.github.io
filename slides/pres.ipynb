{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376dc998",
   "metadata": {
    "id": "376dc998"
   },
   "source": [
    "# `DeepComplete`\n",
    "\n",
    "## Generative Adversarial Autoencoders for Multiple Imputation\n",
    "\n",
    "### Luna Bellitto, Will de Melo, Erika Garza-Elorduy, Emily Han, Umberto Mignozzetti, and Asad Tariq (UCSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbcd26",
   "metadata": {
    "id": "49dbcd26"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- Missing data has been a persistent issue in empirical political science.\n",
    "\n",
    "- It leads to biased results and compromises the validity of analyses (King et al., 2001; Zhang, 2015; Lall 2016, 2017; Stavseth, 2019; Lall and Robinson, 2022).\n",
    "\n",
    "- Traditional multiple imputation methods:\n",
    "    1. Fail to capture non-linear data representations.\n",
    "    1. Often operate under parametric assumptions.\n",
    "    1. Sometimes use MCMC, which may take considerably longer to estimate (Kropco et al. 2014).\n",
    "\n",
    "- Recently, Deep Learning advances have provided easy-to-use and fast new methods for multiple imputation (Gondara and Wang 2018; Ma et al. 2020; Lall and Robinson, 2022; Lall and Robinson, 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ae147",
   "metadata": {
    "id": "da8ae147"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- In this project, we present a new methodology for performing multiple imputations: `DeepComplete`\n",
    "\n",
    "- `DeepComplete` uses adversarial autoencoders (Makhzani et al., 2014) for multiple imputations, attacking the encoder and the full generative process.\n",
    "  \n",
    "- **Key Features**:\n",
    "    1. Combines autoencoders with adversarial networks to map original data into a low-dimensional latent space ($l = 2$).\n",
    "    1. Uses a Wasserstein GAN discriminator (Guljarani et al. 2017) to refine and stabilize the latent space, helping a robust data reconstruction.\n",
    "    1. Perform completions not only using denoising, but also actively engaging with the latent space structure (Convex Hull completion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e72d26",
   "metadata": {
    "id": "b8e72d26"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- To evaluate `DeepComplete`, we used a 3631 sample from the 2020 ANES survey, with 27 variables and the MNIST hand written digits data.\n",
    "  - Since the MNIST data is still running, we will present only the ANES results in here.\n",
    "\n",
    "- We created 720 datasets varying the amont of missingness, the type of missingness (MAR x MCAR).\n",
    "\n",
    "- We show that `DeepComplete`:\n",
    "    1. Presents an MSE reconstruction of 2.07 (SE=0.026), which is slightly better than `MIDAS` (2.12, SE=0.027).\n",
    "    2. Using the convex hull multiple imputation approach has a smaller MSE (1.99, SE=0.037) than conventional sampling (2.15, SE=0.036).\n",
    "    3. Deterministic latent structures perform slightly better (2.05, SE=0.037) than Gaussian latent structures (2.10, SE=0.037).\n",
    "    4. (expected) Imputations on MCAR perform better (1.94, SE=0.037) than MAR (2.20, SE=0.035)\n",
    "    5. (expected) Lower the missing amounts result in better imputation quality.\n",
    "    \n",
    "- But `DeepComplete` still performs 1.7 times slowlier than `MIDAS`, most likely because it fits two neural networks at each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29792d28",
   "metadata": {
    "id": "29792d28"
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "- Autoencoders were proposed by Hinton and Salakhutdinov (2006) as a type of \"non-linear\" PCA, using neural networks.\n",
    "\n",
    "- Since then, many types of Autoencoders have been proposed, for applications such as anomaly detection, data compressing, and data reconstruction.\n",
    "\n",
    "- The basic architecture of an autoencoder:\n",
    "\n",
    "  1. Receives $k$-dimensional data ($x \\in \\mathbb{R}^k$) and maps it into an $f_{\\text{enc}}: \\mathbb{R}^k \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^l$, encoding it into a $l$-dimensional manifold ($z \\in \\mathbb{R}^l$).\n",
    "\n",
    "  2. Take the $l$-dimensional latent representation ($z \\in \\mathbb{R}^l$) and maps back into the original space by passing a transformation $f_{\\text{dec}}: \\mathbb{R}^l \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^k$.\n",
    "\n",
    "- Any functional forms $(f_{\\text{enc}}, f_{\\text{dec}})$ may be used, and here we use neural networks, since they have powerful properties when it comes to detect high non-linearities in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440538b",
   "metadata": {
    "id": "3440538b"
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "- However, encoding data in way has a few disadvantages. and to fix some of these problems, regularizations have been proposed so that the latent space is reshaped in a way that stabilizes training.\n",
    "\n",
    "- Among the three most important regularizations:\n",
    "    1. **Variational autoencoders**: Imposing a KL divergence penalty whenever the latent space deviates from a given parametric distribution (Kingma and Welling, 2013)\n",
    "    2. **Moment-matching networks**: Ensures that as many moments of the latent distribution match pre-determined values (Li et al., 2015)\n",
    "    3. **Adversarial attacks**: Way to regularize the space by creating a zero-sum game between two neural networks (Makhzani et al. 2015)\n",
    "\n",
    "- A fourth regularization, most common in corrupt data applications is **Denoising** (Vincent et al. 2010).\n",
    "    - It consists in corrupt fractions of the input and use the uncorrupted data as benchmarking in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414880b",
   "metadata": {
    "id": "a414880b"
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "- In `DeepComplete`, we use **Denoising** + **Adversarial attacks**.\n",
    "\n",
    "- **Denoising**:\n",
    "    - After finish training the autoencoder, I reduce original learning rate (0.001) by a factor of one thousand (0.000001)\n",
    "    - Propose a corrupt schedule of (5%, 10%, 15%, and 20%).\n",
    "    - Refit the algorithm\n",
    "\n",
    "- The denoising process have a two-fold benefit:\n",
    "    1. Avoids identity-mapping (overfitting)\n",
    "    2. Improves the stability of the reconstructed space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a2d0b",
   "metadata": {
    "id": "3f8a2d0b"
   },
   "source": [
    "## Adversarial Autoencoders\n",
    "\n",
    "- **Adversarial Autoencoders**:\n",
    "    - Adversarial Networks are neural networks that are trained separately, but with loss functions linked.\n",
    "    \n",
    "    - In `DeepComplete`, the adversary (called **discriminator**) is 1-Lipschitz function $g_\\text{disc}: \\mathbb{R}^l \\times \\mathbb{R}^p \\rightarrow [-\\infty, \\infty]$ that receives data from the encoder and from a given distribution, and scores it.\n",
    "    \n",
    "    - We then tie these networks together:\n",
    "    \n",
    "        1. Training the loss function of the **discriminator** to maximize accuracy (getting better in telling if a set of points came from the preset distribution)\n",
    "        \n",
    "        2. Training the loss function of the **encoder** to try to fool the discriminator.\n",
    "\n",
    "- The equilibrium of this game makes the **encoder** very skilled in producing results close by the preset distribution.\n",
    "\n",
    "- This has regularizing effects that go beyond mode selection, improving the shape of the latent space and its contiguity (Makhzani et al. 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f38960",
   "metadata": {
    "id": "04f38960"
   },
   "source": [
    "## Adversarial Autoencoders\n",
    "\n",
    "The full `DeepComplete` setup is as follows:\n",
    "\n",
    "<img src=\"aae.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55248f85",
   "metadata": {
    "id": "55248f85"
   },
   "source": [
    "## Adversarial Autoencoders for Multiple Imputation\n",
    "\n",
    "- But most applications of autoencoders consider the latent space as a *step in the process*.\n",
    "\n",
    "- They rarely engage with it, unless for penalizing them based on a given distribution, with the objective of improving algorithm convergence.\n",
    "\n",
    "- The latent space are basically considered to be blobs of information.\n",
    "\n",
    "<img src=\"deter1.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe7b0c",
   "metadata": {
    "id": "4afe7b0c"
   },
   "source": [
    "## Adversarial Autoencoders for Multiple Imputation\n",
    "\n",
    "The \"blob assumption\" may be correct for some variables:\n",
    "\n",
    "<img src=\"deter2.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec17602",
   "metadata": {
    "id": "5ec17602"
   },
   "source": [
    "## Adversarial Autoencoders for Multiple Imputation\n",
    "\n",
    "But not for others:\n",
    "\n",
    "<img src=\"deter3.png\" width=\"600\"/>\n",
    "\n",
    "Therefore, we can use information from the latent space to improve the imputation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911578e",
   "metadata": {
    "id": "7911578e"
   },
   "source": [
    "## Adversarial Autoencoders for Multiple Imputation\n",
    "\n",
    "- In this project, we created a method to engage with the latent space, using proximate values of an intended generated observation, to improve the outcome of the multiple imputation.\n",
    "\n",
    "- It consists in:\n",
    "    1. Select $k$ observations close to the original data (disconsidering the missing values)\n",
    "    2. Find the latent representation of these $k$ values\n",
    "    3. Compute the Convex-Hull formed by these $k$ values\n",
    "    4. Sample a reconstruction (the data to impute) from this Convex-Hull.\n",
    "\n",
    "We call this procedure `Convex Hull` imputation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242ce74",
   "metadata": {
    "id": "a242ce74"
   },
   "source": [
    "## Simulations\n",
    "\n",
    "We perform simulations using the 2020 ANES dataset. We selected complete observations for 3631 values and 27 variables:\n",
    "\n",
    "<img src=\"tabvars.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3c74f",
   "metadata": {
    "id": "e0c3c74f"
   },
   "source": [
    "## Simulations\n",
    "\n",
    "We perform the following simulations:\n",
    "\n",
    "<img src=\"tabsims.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e818b",
   "metadata": {
    "id": "f91e818b"
   },
   "source": [
    "## Results\n",
    "\n",
    "<img src=\"sims11.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0cc89",
   "metadata": {
    "id": "c1b0cc89"
   },
   "source": [
    "## Results\n",
    "\n",
    "<img src=\"sims22.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b21b6c",
   "metadata": {
    "id": "28b21b6c"
   },
   "source": [
    "## Results\n",
    "\n",
    "<img src=\"sims3.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172d2b6",
   "metadata": {
    "id": "0172d2b6"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- **DeepComplete**:\n",
    "  - Introduces an adversarial autoencoder technique for multiple imputations.\n",
    "  - Combines autoencoders with adversarial networks for robust data reconstruction.\n",
    "  - Utilizes convex hull sampling to improve imputation accuracy.\n",
    "\n",
    "- **Performance**:\n",
    "  - Slightly better imputation performance compared to traditional methods like MIDAS (but slowlier).\n",
    "  - Effective in handling both missing completely at random (MCAR) and missing at random (MAR) patterns.\n",
    "\n",
    "- **Contributions**:\n",
    "  - Provides a flexible and effective solution for handling missing data in political science datasets.\n",
    "  - Enhances data completeness and analytical robustness.\n",
    "\n",
    "- **Next Steps**:\n",
    "  - Get the MNIST to work and find a text example.\n",
    "  - Build a method to evaluate the quality of the imputation using the latent space."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
